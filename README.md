# 📈 메타모델 기반 앙상블 기법: Stacking vs. Blending 성능 비교

이 프로젝트는 메타 모델(Meta-model) 기반의 대표적인 앙상블 기법인 **스태킹(Stacking)** 과 **블렌딩(Blending)** 의 성능을 비교 분석하여 

주어진 문제에 대한 최적의 모델링 전략을 탐구하는 것을 목표로 합니다.

## 🎯 주제 선정 배경

데이콘의 Toss 앱 내 광고 클릭 예측 프로젝트를 진행하며 단일 모델의 성능을 뛰어넘는 방법에 대해 고민하게 되었습니다. 

그 해결책으로 **메타 모델 기반 앙상블 기법**이 성능 향상의 핵심이 될 수 있음을 발견했습니다. 

특히, 유사하면서도 명확한 차이가 있는 스태킹과 블렌딩의 성능을 직접 구현하고 비교해보고자 이 프로젝트를 시작하게 되었습니다.

---

## 📚 핵심 개념 정리

### 앙상블 (Ensemble)

> 여러 개의 머신러닝 모델을 결합하여 각 모델에서 발생할 수 있는 오류를 줄이고 전체적인 예측 성능을 향상시키는 기법입니다.

#### 앙상블을 사용하는 이유

1.  **성능 향상**: 단일 모델이 가질 수 있는 오류나 편향을 여러 모델을 결합함으로써 보완하여 전반적인 예측 정확도를 높입니다.
2.  **과적합 방지**: 여러 모델의 결과를 종합(평균, 투표 등)함으로써 특정 훈련 데이터에 과도하게 최적화되는 것을 막아 모델의 안정성과 일반화 성능을 향상시킵니다.

#### 대표적인 앙상블 기법

| 구분          | **배깅 (Bagging)** | **부스팅 (Boosting)** | **스태킹 (Stacking)** |
| :------------ | :----------------------------------------------- | :--------------------------------------------------- | :--------------------------------------------------- |
| **핵심 아이디어** | 병렬 학습 후 결과 집계                           | 순차적 학습을 통해 이전 모델의 오류를 보완             | 여러 모델의 예측값을 새로운 특성으로 활용해 메타 모델 학습 |
| **학습 방식** | 병렬 (Parallel)                                  | 순차 (Sequential)                                    | 병렬 (Base Models) → 단일 (Meta Model)               |
| **대표 모델** | 랜덤 포레스트 (Random Forest)                    | XGBoost, LightGBM                                    | 기본 모델 조합 + 메타 모델                             |

### 메타 모델 (Meta-model)

> **'모델을 학습하는 모델'**
> 
> 여러 기본 모델(Base Model)들의 예측 결과를 입력 데이터로 받아 최종 예측을 수행하는 상위 모델을 의미합니다.

<img alt="Image" src="https://github.com/user-attachments/assets/8cd3fda4-58d4-4b06-915a-43349fa61ffb" width="600" style="height:auto;" alt="Image">

---

## 🆚 스태킹 (Stacking) vs. 블렌딩 (Blending)

두 기법의 가장 핵심적인 차이는 **"메타 모델을 학습시키기 위한 훈련 데이터를 어떻게 만드는가"** 에 있습니다.

| **스태킹 (Stacking)** | **블렌딩 (Blending)** |
| :----------------------------------------------------- | :----------------------------------------------------- |
| **K-Fold 교차 검증 (Cross-Validation)** 방식      | **Hold-out** 방식                                 |
| 훈련 데이터 전체를 활용하여 메타 모델 학습 데이터를 생성 | 훈련 데이터를 일정 비율로 나누어(Train/Validation) 생성    |
| 데이터 손실이 적지만, 시간이 오래 걸릴 수 있음         | 구현이 간단하고 빠르지만, 메타 모델 학습 데이터가 적음 |
| [스태킹 기반 메타모델 생성 과정]<img width="1133" height="565" alt="Image" src="https://github.com/user-attachments/assets/54546414-8c69-48c5-9110-f0061dc876aa" />                   | [블렌딩 기반 메타모델 생성 과정]<img width="824" height="528" alt="Image" src="https://github.com/user-attachments/assets/89832e7d-c050-4695-a171-ebae2aed54df" />                    |

---

## ⚙️ 구현 과정

### 1. 데이터셋

* **데이터**: Kaggle의 [**사기 거래 탐지 (Fraud Transaction Detection)**](https://www.kaggle.com/datasets/sanskar457/fraud-transaction-detection/) 데이터
* **데이터 크기**: 약 175만 건
* **주요 특징**: `TX_AMOUNT`, `TX_TIME_SECONDS`, `TX_TIME_DAYS`, `TX_HOUR`, `TX_DAY_OF_WEEK`
* **예측 목표 (Target)**: `TX_FRAUD` (사기 여부)
* **데이터 불균형**: 합법 거래와 사기 거래의 비율이 **86.5 : 13.5**로 불균형한 데이터

### 2. 사용 모델

* **기본 모델 (Base Models)**
    * **LightGBM**: 부스팅 계열로, 과적합 방지에 효과적이고 빠른 속도를 가집니다.
    * **RandomForest**: 배깅 계열로, 모델의 편향을 줄이고 안정적인 성능을 보입니다.
* **메타 모델 (Meta Model)**
    * **Logistic Regression**: 기본 모델들의 예측 결과에 대한 최적의 가중치를 학습하는 데 있어, 모델 자체가 단순하여 과적합 위험이 적고 안정적이기 때문에 선정했습니다.

### 3. 평가지표

데이터가 불균형하기 때문에, 소수 클래스(사기 거래)를 얼마나 잘 탐지하는지를 종합적으로 평가하기 위해 다음 지표를 선정했습니다.

* **F1-Score**: 정밀도(Precision)와 재현율(Recall)의 조화 평균으로, 불균형 데이터에서 모델 성능을 정확하게 평가할 수 있습니다.
* **ROC-AUC**: 모델이 '정상(0)'과 '사기(1)'를 얼마나 잘 구별하는지에 대한 전반적인 판별 능력을 평가합니다.
* **PR-AUC (AUPRC)**: 소수 클래스를 얼마나 정밀하게 잘 찾아내는지에 더 집중하는 지표로, 불균형 데이터 평가에 매우 중요합니다.



### 4. 1차 구현: 기본 모델 성능 비교
동일한 하이퍼파라미터를 사용하는 기본 모델(LGBM, RF)을 기반으로 스태킹과 블렌딩을 구현하고 성능을 비교했습니다.

| 구분 | 스태킹 (Stacking) | 블렌딩 (Blending) |
| :---: | :---: | :---: |
| **구현 코드** | <img width="827" alt="Stacking Code" src="https://github.com/user-attachments/assets/289f381c-7381-4055-9115-a02077a09ac4" /> | <img width="767" alt="Blending Code" src="https://github.com/user-attachments/assets/f05d43a0-f113-4996-82f1-a7e70ed3b8fe" /> |
| **실행 결과** | <img width="553" alt="Stacking Result" src="https://github.com/user-attachments/assets/53295df4-d905-427e-bba5-ddd63a8ceb4e" /> | <img width="541" alt="Blending Result" src="https://github.com/user-attachments/assets/95bd40e1-4315-49c4-a86d-effc14f979fe" /> |

#### 📊 1차 구현 고찰
실행 결과, **두 기법 간의 유의미한 성능 차이는 발견되지 않았습니다.** 이는 튜닝되지 않았음에도 불구하고 기본 모델로 사용된 LightGBM과 RandomForest 자체가 이미 강력한 성능을 가지고 있어, 메타 데이터를 생성하는 방식의 차이가 최종 성능에 미치는 영향이 미미했기 때문으로 예측합니다.

---
### 5. 1차 구현의 한계점 및 발전방향

#### 5-1. 프로젝트의 한계점 (이슈)
* **하이퍼파라미터 최적화 부재**: 각 기본 모델에 대한 별도의 튜닝을 진행하지 않아 모델 본연의 최고 성능을 이끌어내지 못했습니다.
* **제한적인 모델 다양성**: 트리 기반 모델 위주로 앙상블을 구성하여 다양성 효과를 극대화하지 못했습니다.

#### 5-2. 향후 발전 방향
* **AutoML을 활용한 하이퍼파라미터 최적화**: `PyCaret`과 같은 AutoML 라이브러리를 사용해 기본 모델의 성능을 극한으로 끌어올립니다.
* **다양한 기본 모델 추가**: K-NN, SVM 등 다른 계열의 모델을 추가하여 최적의 조합을 탐색합니다.

---
### 6. 2차 구현: AutoML을 통한 성능 개선 및 고찰

'발전방향'에서 계획했던 대로 `PyCaret`을 사용하여 기본 모델들의 하이퍼파라미터를 최적화한 후, 다시 성능을 비교했습니다.

#### 6-1. 개선 시도와 예상치 못한 문제 발견
데이터 불균형을 고려하여 주요 성능지표를 **AUPRC**와 **F1-Score (macro avg)** 로 설정하고 최적화를 진행했습니다.

| 구분 | 스태킹 (Stacking) | 블렌딩 (Blending) |
| :---: | :---: | :---: |
| **최적화 이전** <br> (AUPRC / F1-Score) | **0.9764** / **0.9898** | **0.9762** / **0.9896** |
| **최적화 이후** <br> (AUPRC / F1-Score) | **0.9726** / **0.9902** | **0.6406** / **0.4658** |

최적화 후 스태킹의 성능은 소폭 향상되었으나, **블렌딩 모델의 성능은 오히려 급격히 하락**하는 예상 밖의 결과가 나타났습니다.

##### 🔍 원인 분석: 평가지표의 함정
> `PyCaret`의 `tune_model()` 함수는 기본적으로 **정확도(Accuracy)**를 기준으로 최적화를 진행합니다.
> 
> 불균형 데이터에서 정확도를 기준으로 지표를 정하면 모델은 소수 클래스(사기)를 포기하고 다수 클래스(정상)만 예측하는 방향으로 학습하여 **AUPRC, F1-Score 같은 핵심 지표가 급락**하게 됩니다. 이 문제가 블렌딩 모델에서 발생한 것으로 분석됩니다.


#### 6-2. 문제 해결 및 최종 결과
위 문제점을 해결하기 위해 `tune_model(..., optimize='AUPRC')`와 같이 **최적화 목표 지표를 명시적으로 설정**하여 실험을 다시 진행했습니다.

<br>
<div align="center">
  <img width="1784" height="233" alt="Image" src="https://github.com/user-attachments/assets/88a68ea0-bbd9-4895-8325-a8885d355b04" />
</div>
<br>

> 그 결과, **스태킹과 블렌딩 모두 AUPRC 0.9726, F1-Score 0.9902로 거의 동일한 최고 성능을 달성**했습니다. <br>
> 두 기법의 결과가 동일하게 나타난 이유는 PyCaret을 통해 개별 기본 모델(LGBM, RF)의 성능이 먼저 최적화되었기 때문입니다. <br>
> 기본 모델들이 이미 정답에 가까운 예측을 하게 되자, 그 예측값을 조합하는 방식의 차이(K-Fold vs Hold-out)가 최종 결과에 거의 영향을 미치지 못하게 된 것입니다. <br>
> 결론적으로, 두 결과가 동일하다는 것은 '기본 모델 최적화'의 영향력이 '메타 데이터 생성 방식(K-Fold vs Hold-out)'의 차이보다 훨씬 컸음을 보여주는 프로젝트였습니다.

---

## 📌 결과

### 7. 최종 결론 및 핵심 인사이트

이번 프로젝트의 전체 구현 과정을 통해 다음과 같은 핵심 인사이트를 얻을 수 있었습니다.

1.  **강력한 기본 모델 앞에서는 기법의 차이가 미미할 수 있다.**
    * 기본 모델의 성능이 이미 충분히 높다면 스태킹과 블렌딩 간의 구조적 차이가 최종 성능에 미치는 영향은 줄어듭니다.

2.  **AutoML 사용 시, 데이터 특성에 맞는 평가지표 설정이 핵심이다.**
    * 특히 불균형 데이터를 다룰 때 '정확도'와 같은 기본 평가지표의 함정에 빠지지 않도록 **AUPRC, F1-Score 등을 최적화 목표로 명시하는 것이 중요**합니다.

3.  **결론적으로, 앙상블의 성능은 기법의 복잡성보다 '기본 모델의 최적화'에 더 크게 의존한다.**
    * 어떤 앙상블 기법을 선택할지 보다는 각 기본 모델의 성능을 하이퍼파라미터 튜닝을 통해 최대한으로 끌어올리는 것이 우선되어야 합니다.
  

### 8. 느낀 점
스태킹 vs 블렌딩이라는 목표를 가지고 프로젝트를 진행하였지만
사용하는 데이터의 특성, 상황에 따라 적절한 모델을 사용하는 것이 가장 좋다는 결론이 나왔다.

이런 결과가 나올 줄 몰랐지만
프로젝트에서는 시간 관계 상 파이캐럿을 사용하여 최적화 진행과정을 하진 못하였지만 
추후에 진행하보니 스태킹의 방식이 최고 성능을 내는 방식임을 다시 확인할 수 있었던 기회였다.
